# -*- coding: utf-8 -*-
"""First Model testing.ipynb

Automatically generated by Colaboratory.

"""

# Documentation gspread
# https://docs.gspread.org/en/latest/user-guide.html

import numpy as np
import pandas as pd
import seaborn as sns
import datetime


!pip install --upgrade -q gspread

import pandas as pd

from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)


# Insert custom functions
!pip install pydrive                             # Package to use Google Drive API - not installed in Colab VM by default
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from oauth2client.client import GoogleCredentials


# Load historical Data
# if you like using Google sheets.
t_sheet_url = "https://docs.google.com/spreadsheets/d/xxxx" #Replace with your own file URL
tab_name = "Train"

worksheet = gc.open_by_url(t_sheet_url)

df = pd.DataFrame.from_records( 
    worksheet.worksheet(tab_name).get_all_values(),
    )

df.columns = df.loc[0].values
df = df.iloc[ 1: ]

type_dic = {
    'bull' : int ,
    'D_MVA200': float , 
    'length': int , 
    'heigth': float , 
    'type': str , 
    'entry': float , 
    'stop': float , 
    'gain': int ,
    'target': float , 
    'stop_trade': float , 
    'resistance_support': str , 
    'MVA200_interact': int,
    'D_target': float,
    'D_stop_trade': float,
    'pair': str
}

df = df.astype( type_dic )

df = df.set_index("date")

df = df.drop(
    columns = [	 
                'entry', 
                'stop', 
                'target', 
                'stop_trade',
               'pair'
                ]
)

# Organice columns for model execution
for col in df.select_dtypes(include=["object"]).columns: # Recorre las columnas que son timpo object

  for i in df[col].unique(): # Recorrer los posibles valores que estan en la columna

    df[col+"_"+i] = np.where(df[col] == i, 1, 0) #Crear nueva columna agretando 1 o 0 dependiendo si tiene o no tiene el valor unico.

  df = df.drop(columns=[col]) # Borrar la columna inicial.

df.head()

#Load ML modules
from sklearn.metrics import confusion_matrix # Para poder crear la matriz de confuci贸n
from sklearn.metrics import (accuracy_score, precision_score,recall_score, roc_auc_score, f1_score) # Para calcular la presici贸n de cada parte de la matriz de confusi贸n
from sklearn.model_selection import train_test_split #Importamos la libreria para definir los datasets de train y test

# Over and undersampler
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler


# Cargar modelos
from sklearn.tree import DecisionTreeClassifier #Importamos el modelo de Arbol
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression # Libreria para regresi贸n Logistica
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
import lightgbm as lgb #pip install lightgbm
import xgboost as xgb #pip install xgboost

# Libreria para realizar el PCA
from sklearn.decomposition import PCA

test_array = [
  "Baja///",
  "Alta///",
  "Baja/Oversample//",
  "Alta/Oversample//",
  "Baja/Undersample//",
  "Alta/Undersample//",
  "Baja//Normal/",
  "Alta//Normal/",
  "Baja/Oversample/Normal/",
  "Alta/Oversample/Normal/",
  "Baja/Undersample/Normal/",
  "Alta/Undersample/Normal/",
  "Baja///PCA",
  "Alta///PCA",
  "Baja/Oversample//PCA",
  "Alta/Oversample//PCA",
  "Baja/Undersample//PCA",
  "Alta/Undersample//PCA",
  "Baja//Normal/PCA",
  "Alta//Normal/PCA",
  "Baja/Oversample/Normal/PCA",
  "Alta/Oversample/Normal/PCA",
  "Baja/Undersample/Normal/PCA",
  "Alta/Undersample/Normal/PCA",
  "///",
  "///",
  "/Oversample//",
  "/Oversample//",
  "/Undersample//",
  "/Undersample//",
  "//Normal/",
  "//Normal/",
  "/Oversample/Normal/",
  "/Oversample/Normal/",
  "/Undersample/Normal/",
  "/Undersample/Normal/",
  "///PCA",
  "///PCA",
  "/Oversample//PCA",
  "/Oversample//PCA",
  "/Undersample//PCA",
  "/Undersample//PCA",
  "//Normal/PCA",
  "//Normal/PCA",
  "/Oversample/Normal/PCA",
  "/Oversample/Normal/PCA",
  "/Undersample/Normal/PCA",
  "/Undersample/Normal/PCA",
]

result = pd.DataFrame(columns=["Dataframe","Modelo",'accuracy','precision',"precision_train",'recall',"f1score","roc","count","matrix"])
c = 0 

for list_test in test_array:

  alza = False
  baja = False
  normalizar = False
  undersample = False
  oversample = False
  double = False
  pca = False

  parameters = list_test.split("/")
  
  if parameters[0] == "Alza":
    alza = True

  if parameters[0] == "Baja":
    baja = True
  
  if parameters[1] == "Oversample":
    undersample = True

  if parameters[1] == "Undersample":
    oversample = True

  if parameters[2] == "Normal":
    normalizar = True

  if parameters[3] == "PCA":
    pca = True  

  # Normalizar columnas
  if normalizar:

    normal = pd.DataFrame()
  
    normal['D_MVA200'] = [np.linalg.norm(df.D_MVA200)]
    df.D_MVA200 = df.D_MVA200/normal['D_MVA200'][0]

    normal['length'] = [np.linalg.norm(df.length)]
    df.length = df.length/normal['length'][0]

    normal['heigth'] = [np.linalg.norm(df.heigth)]
    df.heigth = df.heigth/normal['heigth'][0]

    normal['heigth'] = [np.linalg.norm(df.heigth)]
    df.heigth = df.heigth/normal['heigth'][0]

  # Dejar solo baja
  if baja:
    df = df[ (df.bull == 0) & (df['type_Double Top'] == 1 ) & (df["resistance_support_none"] == 0 ) ]

  # Dejar solo Alta
  if alza:
    df = df[ (df.bull == 1) & (df['type_Double Bottom'] == 1 ) & (df["resistance_support_none"] == 0 ) ]

  # Creat data sets
  x = df.drop(columns = 'gain') #Elimino de mi dataset la variable a predecir
  y = df['gain'].astype(int) #Defino el Target


  if pca:
    pca = PCA()
    components = pca.fit_transform( x )
    
    pca_array = []
    a = 0
    while a < len( x.columns ):
      pca_array += [ "PC"+str(a + 1) ]
      a += 1
    
    npc = np.array(components)
    dfc = pd.DataFrame(npc, columns=pca_array)

    x = dfc[["PC1","PC2"]]


  #Generate 200 random numbers between 10 and 1000
  import random
  randomlist = random.sample(range(10, 100), 5)
  for a in randomlist:

    # Crear los dataset de entrenamiento y test
    X_train, X_test, y_train, y_test = train_test_split(
        x, 
        y,
        stratify = y, # Matener una cantidad similar de observaciones positivas en cada data set
        test_size = 0.20, 
        random_state = a
        )
    
    if oversample:
      # define oversampling strategy
      oversample = RandomOverSampler(
          sampling_strategy='minority',
          random_state = a
          )
      # fit and apply the transform
      X_over, y_over = oversample.fit_resample(X_train, y_train)
      X_train = X_over
      y_train = y_over

    if undersample:
      # define undersample strategy
      undersample = RandomUnderSampler(
          sampling_strategy='majority',
          random_state = a
          )
      # fit and apply the transform
      X_over, y_over = undersample.fit_resample(X_train, y_train)
      X_train = X_over
      y_train = y_over

      
    ############ Tree

    #Creamos la estructura del arbol
    tree_fx = DecisionTreeClassifier(
        max_depth=2,
        min_samples_leaf = 100,
        class_weight = "balanced",
        random_state = 12,
        criterion = "entropy",
        splitter = "random",
        max_features = None
        ) 

    # Entrenamos el modelo utilizando tree_fx
    tree_fx.fit(
        X_train,
        y_train) #Entrenamos el modelo

    y_test_pred = tree_fx.predict(X_test) #Prediccion en Test
    y_train_pred = tree_fx.predict(X_train) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "DecisionTreeClassifier", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ RandomForestClassifier

    #Creamos un random forest!
    RandomeForest_fx = RandomForestClassifier(
        n_estimators = 50,
        random_state = 12,
        class_weight = "balanced",
        criterion = "entropy",
        max_depth = 30,
        min_weight_fraction_leaf = 0.0,
        max_features = "log2",
        max_leaf_nodes = None,
        min_impurity_decrease = 0.0,
        ccp_alpha = 0.0005
        )

    RandomeForest_fx.fit(
        X_train,
        y_train) #Entrenamos el modelo

    y_test_pred = RandomeForest_fx.predict(X_test) #Prediccion en Test
    y_train_pred = RandomeForest_fx.predict(X_train) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "RandomForestClassifier", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ KNeighborsClassifier

    knn = KNeighborsClassifier(
        n_neighbors=10, # Cuantos puntos a tener en cuenta.
        )


    knn.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = knn.predict(X_train) #Prediccion en Train
    y_test_pred = knn.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "KNeighborsClassifier", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ LogisticRegression


    # Paso 4: Creamos una instancia de la Regresi贸n Log铆stica
    regresion = LogisticRegression(
        random_state=12,
        class_weight = "balanced",
        solver = "saga",
        max_iter = 100,
        multi_class = "auto",
        warm_start = False,
        penalty='l2'
    )

    # Paso 5: Entrenamos la regresi贸n log铆stica con los datos de entrenamiento
    regresion.fit(
        X_train,
        y_train) #Entrenamos el modelo

    y_train_pred = regresion.predict(X_train) #Prediccion en Train
    y_test_pred = regresion.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "LogisticRegression", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ SVC
    
    SVC_model = SVC(
        class_weight = "balanced",
        random_state=12,
        C = 150,
        kernel = "poly", # Poly 27.94
        degree = 2 # 1 25.11
        )

    # Entrenamos el modelo utilizando tree_fx
    SVC_model.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = SVC_model.predict(X_train) #Prediccion en Train
    y_test_pred = SVC_model.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "SVC", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ GaussianNB

    gnb = GaussianNB()

    # Entrenamos el modelo utilizando tree_fx
    gnb.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = gnb.predict(X_train) #Prediccion en Train
    y_test_pred = gnb.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "GaussianNB", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ BernoulliNB

    bnb = BernoulliNB()

    # Entrenamos el modelo utilizando tree_fx
    bnb.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = bnb.predict(X_train) #Prediccion en Train
    y_test_pred = bnb.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "BernoulliNB", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ AdaBoostClassifier

    adaboost = AdaBoostClassifier(
        n_estimators=100, 
        random_state=0)

    # Entrenamos el modelo utilizando tree_fx
    adaboost.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = adaboost.predict(X_train) #Prediccion en Train
    y_test_pred = adaboost.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "AdaBoostClassifier", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ GradientBoostingClassifier

    gbrt = GradientBoostingClassifier(random_state = 12)

    # Entrenamos el modelo utilizando tree_fx
    gbrt.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = gbrt.predict(X_train) #Prediccion en Train
    y_test_pred = gbrt.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "GradientBoostingClassifier", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ lightgbm

    clf = lgb.LGBMClassifier(
        random_state = 0
    )

    # Entrenamos el modelo
    clf.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = clf.predict(X_train) #Prediccion en Train
    y_test_pred = clf.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "lightgbm", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ xgboost

    clf_xgb = xgb.XGBClassifier()

    # Entrenamos el modelo
    clf_xgb.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = clf_xgb.predict(X_train) #Prediccion en Train
    y_test_pred = clf_xgb.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "xgboost", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    # Crear los dataset de entrenamiento y test
    X_train, X_test, y_train, y_test = train_test_split(
        x, 
        y,
        stratify=y, # Matener una cantidad similar de observaciones positivas en cada data set
        test_size=0.30, 
        random_state=214
        )
    
    if oversample:
      # define oversampling strategy
      oversample = RandomOverSampler(
          sampling_strategy='minority',
          random_state = 456
          )
      # fit and apply the transform
      X_over, y_over = oversample.fit_resample(X_train, y_train)
      X_train = X_over
      y_train = y_over

    if undersample:
      # define undersample strategy
      undersample = RandomUnderSampler(
          sampling_strategy='majority',
          random_state = 6921
          )
      # fit and apply the transform
      X_over, y_over = undersample.fit_resample(X_train, y_train)
      X_train = X_over
      y_train = y_over
    
    ############ Tree

    #Creamos la estructura del arbol
    tree_fx = DecisionTreeClassifier(
        max_depth=2,
        min_samples_leaf = 100,
        class_weight = "balanced",
        random_state = 12,
        criterion = "entropy",
        splitter = "random",
        max_features = None
        ) 

    # Entrenamos el modelo utilizando tree_fx
    tree_fx.fit(
        X_train,
        y_train) #Entrenamos el modelo

    y_test_pred = tree_fx.predict(X_test) #Prediccion en Test
    y_train_pred = tree_fx.predict(X_train) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "DecisionTreeClassifier", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ RandomForestClassifier

    #Creamos un random forest!
    RandomeForest_fx = RandomForestClassifier(
        n_estimators = 50,
        random_state = 12,
        class_weight = "balanced",
        criterion = "entropy",
        max_depth = 30,
        min_weight_fraction_leaf = 0.0,
        max_features = "log2",
        max_leaf_nodes = None,
        min_impurity_decrease = 0.0,
        ccp_alpha = 0.0005
        )

    RandomeForest_fx.fit(
        X_train,
        y_train) #Entrenamos el modelo

    y_test_pred = RandomeForest_fx.predict(X_test) #Prediccion en Test
    y_train_pred = RandomeForest_fx.predict(X_train) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "RandomForestClassifier", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ KNeighborsClassifier

    knn = KNeighborsClassifier(
        n_neighbors=10, # Cuantos puntos a tener en cuenta.
        )


    knn.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = knn.predict(X_train) #Prediccion en Train
    y_test_pred = knn.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "KNeighborsClassifier", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ LogisticRegression


    # Paso 4: Creamos una instancia de la Regresi贸n Log铆stica
    regresion = LogisticRegression(
        random_state=12,
        class_weight = "balanced",
        solver = "saga",
        max_iter = 100,
        multi_class = "auto",
        warm_start = False,
        penalty='l2'
    )

    # Paso 5: Entrenamos la regresi贸n log铆stica con los datos de entrenamiento
    regresion.fit(
        X_train,
        y_train) #Entrenamos el modelo

    y_train_pred = regresion.predict(X_train) #Prediccion en Train
    y_test_pred = regresion.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "LogisticRegression", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ SVC
    
    SVC_model = SVC(
        class_weight = "balanced",
        random_state=12,
        C = 150,
        kernel = "poly", # Poly 27.94
        degree = 2 # 1 25.11
        )

    # Entrenamos el modelo utilizando tree_fx
    SVC_model.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = SVC_model.predict(X_train) #Prediccion en Train
    y_test_pred = SVC_model.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "SVC", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ GaussianNB

    gnb = GaussianNB()

    # Entrenamos el modelo utilizando tree_fx
    gnb.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = gnb.predict(X_train) #Prediccion en Train
    y_test_pred = gnb.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "GaussianNB", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ BernoulliNB

    bnb = BernoulliNB()

    # Entrenamos el modelo utilizando tree_fx
    bnb.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = bnb.predict(X_train) #Prediccion en Train
    y_test_pred = bnb.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "BernoulliNB", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ AdaBoostClassifier

    adaboost = AdaBoostClassifier(
        n_estimators=100, 
        random_state=0)

    # Entrenamos el modelo utilizando tree_fx
    adaboost.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = adaboost.predict(X_train) #Prediccion en Train
    y_test_pred = adaboost.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "AdaBoostClassifier", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ GradientBoostingClassifier

    gbrt = GradientBoostingClassifier(random_state = 12)

    # Entrenamos el modelo utilizando tree_fx
    gbrt.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = gbrt.predict(X_train) #Prediccion en Train
    y_test_pred = gbrt.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "GradientBoostingClassifier", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ lightgbm

    clf = lgb.LGBMClassifier(
        random_state = 0
    )

    # Entrenamos el modelo
    clf.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = clf.predict(X_train) #Prediccion en Train
    y_test_pred = clf.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "lightgbm", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

    ############ xgboost

    import xgboost as xgb #pip install xgboost

    clf_xgb = xgb.XGBClassifier()

    # Entrenamos el modelo
    clf_xgb.fit(
        X_train,
        y_train) #Entrenamos el modelo


    y_train_pred = clf_xgb.predict(X_train) #Prediccion en Train
    y_test_pred = clf_xgb.predict(X_test) #Prediccion en Test

    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    precision_train = precision_score(y_train, y_train_pred)
    recall = recall_score(y_test, y_test_pred)
    f1score = f1_score(y_test, y_test_pred)
    roc = roc_auc_score(y_test, y_test_pred)
    matriz = confusion_matrix(y_test, y_test_pred)

    count = pd.DataFrame(y_test_pred).value_counts()
    if count.index.size > 1:
      count =  count.loc[1].values[0]
    else:
      count = 0

    result.loc[c] =  [ list_test , "xgboost", accuracy, precision, precision_train, recall, f1score, roc, count , matriz ]
    c += 1

# Include difference column into DataSet
result["precision_difference"] =  ( result.precision - result.precision_train )/ result.precision_train
result = result.dropna()
result.precision_difference = result.precision_difference.replace([np.inf, -np.inf],0)
result.matrix = result.matrix.astype(str)           

# Save data into t-sheet
t_sheet_url = "https://docs.google.com/spreadsheets/XXX" # Replace with your own file URL
tab_name = "Initial_results"

worksheet_result = gc.open_by_url(t_sheet_url)

worksheet_result.worksheet(tab_name).update( 
    "A1",
    [result.columns.values.tolist()] + result.values.tolist()
    )

result

df

